{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is a Decision Tree, and how does it work?"
      ],
      "metadata": {
        "id": "NxN9ypO__iiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A0f3OQTeACTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. A Decision Tree is a supervised learning algorithm used for classification and regression. It works by recursively splitting data based on the feature that best separates it, forming a tree-like structure. Each internal node represents a decision, branches represent possible outcomes, and leaf nodes contain final predictions."
      ],
      "metadata": {
        "id": "jUdYDQ20_lAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are impurity measures in Decision Trees?"
      ],
      "metadata": {
        "id": "LY3fpf5lADv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Impurity measures in Decision Trees determine how mixed a node is. Lower impurity means better splits.\n",
        "\n",
        "Common Impurity Measures :\n",
        "\n",
        "Gini Impurity (used in CART) ‚Ä¢ Measures how often a randomly chosen element would be misclassified. ‚Ä¢ Formula: ùê∫ ùëñ ùëõ ùëñ = 1 ‚àí ‚àë ùëù ùëñ 2 Gini=1‚àí‚àëp i 2‚Äã\n",
        "\n",
        "Entropy (used in ID3, C4.5)\n",
        "\n",
        "‚Ä¢ Measures the uncertainty in data. ‚Ä¢ Formula: ùê∏ ùëõ ùë° ùëü ùëú ùëù ùë¶ = ‚àí ‚àë ùëù ùëñ log ‚Å° 2 ùëù ùëñ Entropy=‚àí‚àëp i‚Äãlog 2‚Äãp i‚Äã\n",
        "\n",
        "Variance Reduction (for regression)\n",
        "‚Ä¢ Measures how much variance decreases after a split."
      ],
      "metadata": {
        "id": "EKNwEDZmAE3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is the mathematical formula for Gini Impurity?"
      ],
      "metadata": {
        "id": "ov9wEv5VAKTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The Gini Impurity formula is:\n",
        "\n",
        "Gini=1‚àí‚àëp i 2‚Äã\n",
        "\n",
        "Where: ‚Ä¢ p i‚Äã= Probability of class ùëñ i in the node. ‚Ä¢ The sum runs over all classes in the node.\n"
      ],
      "metadata": {
        "id": "jtTsCg9wAOTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the mathematical formula for Entropy?"
      ],
      "metadata": {
        "id": "SQyZvs6zAUei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The Entropy formula is:\n",
        "\n",
        "Entropy=‚àí‚àëp i‚Äãlog 2‚Äãp i‚Äã\n",
        "\n",
        "Where: ‚Ä¢ p i‚Äã= Probability of class ùëñ i in the node. ‚Ä¢ The sum runs over all classes in the node."
      ],
      "metadata": {
        "id": "E8A0GVGfAbjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "zSkqiHYJAdma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Information Gain (IG) measures how much uncertainty (entropy) is reduced after a split in a Decision Tree. It helps choose the best feature to split on.\n",
        "\n",
        "Formula:\n",
        "\n",
        "IG=Entropy(parent)‚àí‚àë( N parent‚Äã\n",
        "\n",
        "N child‚Äã\n",
        "\n",
        "‚Äã√óEntropy(child))\n",
        "\n",
        "Where: ‚Ä¢ Higher IG means a better split. ‚Ä¢ The feature with the highest IG is selected for splitting."
      ],
      "metadata": {
        "id": "kokhZGoOAqW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "k6C8nwyhArlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Gini Impurity vs. Entropy:\n",
        "\n",
        "‚Ä¢ Gini Impurity measures misclassification probability (faster to compute). ‚Ä¢ Entropy measures data uncertainty (slower due to log calculations). ‚Ä¢ Gini is used in CART, while Entropy is used in ID3, C4.5. ‚Ä¢ Both give similar results, but Gini is more efficient."
      ],
      "metadata": {
        "id": "aEA05uImAvXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What is the mathematical explanation behind Decision Trees?"
      ],
      "metadata": {
        "id": "eT7cJ1y8AyYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Mathematical Explanation of Decision Trees\n",
        "\n",
        "Splitting: Choose the best feature using Information Gain (IG) or Gini Impurity.\n",
        "Recursive Partitioning: Keep splitting until stopping conditions are met.\n",
        "Prediction:\n",
        "‚Ä¢ Classification: Majority class in the leaf node. ‚Ä¢ Regression: Mean of values in the leaf node.\n",
        "\n",
        "Pruning: Reduces overfitting by trimming weak branches.\n",
        "Key Idea: Decision Trees use entropy reduction or impurity minimization to make hierarchical decisions!"
      ],
      "metadata": {
        "id": "DylKlv6HA2rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "7l5oPQzhA4N4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Pre-Pruning stops Decision Tree growth early to prevent overfitting.\n",
        "\n",
        "Stops Splitting If:\n",
        "\n",
        "‚Ä¢ Max depth is reached. ‚Ä¢ Min samples per leaf are too few. ‚Ä¢ gain is too low."
      ],
      "metadata": {
        "id": "rwFvBUKCA7CY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is Post-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "Vrb0wN3VA-Dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Post-Pruning trims a fully grown Decision Tree after training to prevent overfitting.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "Grow the full tree.\n",
        "Use validation data to evaluate branches.\n",
        "Remove weak nodes that don‚Äôt improve performanc"
      ],
      "metadata": {
        "id": "chd9g4T8BB2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What is the difference between Pre-Pruning and Post-Pruning?"
      ],
      "metadata": {
        "id": "SgDvRlNRBZ7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Pre-Pruning vs. Post-Pruning\n",
        "\n",
        "‚Ä¢ Pre-Pruning stops tree growth early (during training). ‚Ä¢ Post-Pruning removes weak branches after training. ‚Ä¢Pre-Pruning is faster but may underfit. ‚Ä¢ Post-Pruning is slower but reduces overfitting better.\n",
        "\n",
        "Pre = Preventive, Post = Corrective!"
      ],
      "metadata": {
        "id": "USnFFqR5Bc4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What is a Decision Tree Regressor?"
      ],
      "metadata": {
        "id": "h51ToIzDBfjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Decision Tree Regressor\n",
        "\n",
        "A Decision Tree Regressor predicts continuous values by splitting data based on features to minimize error.\n",
        "\n",
        "Key Points:\n",
        "\n",
        "‚Ä¢ Uses MSE or MAE to find the best split. ‚Ä¢ Predicts by taking the average of values in each leaf node. ‚Ä¢ Handles non-linearity but can overfit without pruning."
      ],
      "metadata": {
        "id": "07cSQNZGBi_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. What are the advantages and disadvantages of Decision Trees"
      ],
      "metadata": {
        "id": "x99aXdqrBnLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Ans. Advantages:\n",
        "\n",
        "‚Ä¢ Easy to understand & interpret. ‚Ä¢Handles both categorical & numerical data. ‚Ä¢ No need for feature scaling. ‚Ä¢ Captures non-linear relationships. ‚Ä¢Automatic feature selection.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "‚Ä¢ Overfitting (needs pruning). ‚Ä¢ Unstable (small changes affect the tree). ‚Ä¢ Biased towards features with many levels. ‚Ä¢ Step-like predictions in regression."
      ],
      "metadata": {
        "id": "aVee43fnBo0s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lSILPCwBnda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Q13. How does a Decision Tree handle missing values?"
      ],
      "metadata": {
        "id": "d7O0hGqmBrN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Handling Missing Values in Decision Trees:\n",
        "\n",
        "Ignore missing values in splits.\n",
        "Impute with mean/median (numerical) or most frequent category (categorical).\n",
        "Surrogate Splitting ‚Äì Use alternative features.\n",
        "Weighting & Probabilities ‚Äì Distribute missing values based on data patterns.\n",
        "Decision Trees handle missing data well!"
      ],
      "metadata": {
        "id": "4RntqWjKBupc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. How does a Decision Tree handle categorical features?"
      ],
      "metadata": {
        "id": "WWmSnW7nBx7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Handling Categorical Features in Decision Trees:\n",
        "\n",
        "Label Encoding ‚Äì Assigns numbers to categories.\n",
        "One-Hot Encoding ‚Äì Converts categories into binary columns.\n",
        "Direct Splitting ‚Äì Splits based on category values.\n",
        "Gini/Entropy Calculation ‚Äì Evaluates best splits.\n",
        "Decision Trees handle categorical data efficiently!"
      ],
      "metadata": {
        "id": "qJs3zIjBB1up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. What are some real-world applications of Decision Trees?"
      ],
      "metadata": {
        "id": "k7y-ElbVB54c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Real-World Applications of Decision Trees:\n",
        "\n",
        "‚Ä¢ Medical Diagnosis ‚Äì Disease prediction. ‚Ä¢ Credit Scoring ‚Äì Loan approval decisions. ‚Ä¢ Customer Segmentation ‚Äì Targeted marketing. ‚Ä¢ Fraud Detection ‚Äì Identifying suspicious transactions. ‚Ä¢ Stock Market Prediction ‚Äì Trend analysis. ‚Ä¢ Churn Prediction ‚Äì Detecting potential customer loss. ‚Ä¢ Quality Control ‚Äì Finding defective products. ‚Ä¢ Spam Filtering ‚Äì Classifying emails."
      ],
      "metadata": {
        "id": "9-KGxy2JB_EP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MZDerxXxB74C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}